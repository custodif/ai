{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_to_French_seq2seq_tf_2.0_withAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSYiDemMKxnJ",
        "colab_type": "text"
      },
      "source": [
        "**DISCLAIMER:**\n",
        "This tutorial demostrates a Neural Machine Translation example based on attention model making use of Tensorflow_addons.seq2seq API. The tutorial takes the ideas & code excerpts from the official tensorflow tutorial as presented in following link \n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md\n",
        "\n",
        "https://github.com/tensorflow/nmt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNNBYh0xgeQ",
        "colab_type": "text"
      },
      "source": [
        "### Datasets\n",
        "For French-English Translation\n",
        "\n",
        "http://www.manythings.org/anki/fra-eng.zip\n",
        "\n",
        "For English-Hindi Translation\n",
        "\n",
        "http://www.manythings.org/anki/hin-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo02O1JtJU5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ASKc-qyIbF",
        "colab_type": "code",
        "outputId": "7d650738-320e-4cfb-c476-54203c9c16a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udq3BXq10NhZ",
        "colab_type": "text"
      },
      "source": [
        "### Download File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-fvT1cC0Qsd",
        "colab_type": "code",
        "outputId": "29b728f7-81d2-447f-9c19-aa8a121a5352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "print(zipfilename)\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)\n",
        "!ls /content/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fra-eng.zip\n",
            "_about.txt  fra-eng.zip  fra.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imc3M7Wb9BQj",
        "colab_type": "code",
        "outputId": "9f6aac69-965e-42b5-ec30-291a0403cdc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /content/fra.txt | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q6Yn0Y18gac",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHmZsuXH3SeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = file.read()\n",
        "    file.close()\n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVM5nnmb81UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3syZHkX83Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For non-english characterset translations such as Hindi, Russian, etc. we keep unicode. \n",
        "def preprocess_sentence(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = s.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "\n",
        "    s = s.rstrip().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGq0hRk9tad",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPv0BItC9wEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(filename, num_samples):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_samples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfQrukoQ9xP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text,Y_text,_  = create_dataset(\"fra.txt\", num_samples=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pzmGOPIypn",
        "colab_type": "code",
        "outputId": "d31f7bcc-2dba-4060-cbdc-0d7bdce163a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(X_text[4000:4001])\n",
        "print(Y_text[4000:4001])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> is this love ? <end>',)\n",
            "('<start> est ce de l amour ? <end>',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3x5kdkhUHa",
        "colab_type": "code",
        "outputId": "bf8ed4e6-6b84-417e-cf7e-853058bc10e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#total samples\n",
        "print(\"Total Samples : \", len(X_text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples :  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbrbbW0uAUVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function to tokenize words into index using inbuild tokenizer vocabulory\n",
        "# important to override filter otherwise it will filter out all punctuation,\n",
        "# plus tabs and line breaks, minus the ' character.\n",
        "def tokenize(input):\n",
        "   tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "   tokenizer.fit_on_texts(input)\n",
        "   sequences = tokenizer.texts_to_sequences(input)\n",
        "  # print(max_len(sequences))\n",
        "   sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "   return  sequences, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vN7jT9Aijw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42RcXojAmdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\n",
        "X , X_tokenizer = tokenize(X_text)\n",
        "Y, Y_tokenizer = tokenize(Y_text)\n",
        "X_train,  X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "Tx = max_len(X)\n",
        "Ty = max_len(Y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhXwcQ7JPMV",
        "colab_type": "code",
        "outputId": "a748ecf0-44e0-46e1-9650-171382099bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Max length English sentence denoted as Tx : \", Tx)\n",
        "print(\"Max length French sentence denoted as Ty: \", Ty)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length English sentence denoted as Tx :  7\n",
            "Max length French sentence denoted as Ty:  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmCJzO38Apcn",
        "colab_type": "code",
        "outputId": "49c125df-0a39-479f-fc22-58d94535bd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X_tokenizer.word_index['<start>'] #'<start>': 2   # tokenize by frequency\n",
        "input_vocab_size = len(X_tokenizer.word_index)+1  # add 1 for 0 sequence character\n",
        "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", input_vocab_size)\n",
        "print(\"output_vocab_size : \" ,output_vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_vocab_size :  1223\n",
            "output_vocab_size :  2374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLXYtWdZBW6a",
        "colab_type": "text"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUu1izSBVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXuxIq6BZuH",
        "colab_type": "code",
        "outputId": "f7ca18a6-6ac6-4c46-9bbc-a8b54e4eb832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG-SZOvCY8mY",
        "colab_type": "code",
        "outputId": "3e32832e-4ae9-48a0-bc21-14d60e9dd15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqFYB-g1HM5o",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow Addons 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL1e3gcnBqcR",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVX6Hyx0S3tw",
        "colab": {}
      },
      "source": [
        "#for LSTM need to initialite Tx hidden and Tx memory state , for GRU only need one of the field\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJTg36aCckr",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer and Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc7yp0FWEedf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00WV880hoVMc",
        "colab_type": "text"
      },
      "source": [
        "###Important note:\n",
        " It's worth pointing out that we divide the loss by batch_size, so our hyperparameters are \"invariant\" to batch_size. Some people divide the loss by (batch_size * num_time_steps), which plays down the errors made on short sentences. \n",
        "\n",
        "More subtly, our hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of 1.0, the latter approach effectively uses a much smaller learning rate of 1 / num_time_steps.\n",
        "\n",
        "### Here, mask is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pT6hAEFEhVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    #skip loss calculation for padding sequences i.e. y = 0 \n",
        "    #[ <start>,How, are, you, today, 0, 0, 0, 0 ....<end>]\n",
        "    #[ 1, 234, 3, 423, 3344, 0, 0 ,0 ,0, 2 ]\n",
        "    # y is a tensor of [batch_size,Ty] . Create a mask when [y=0]\n",
        "    # mask the loss when padding sequence appears in the output sequence\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Swb9mvflqsQ",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxPNQirBtsh",
        "colab_type": "text"
      },
      "source": [
        "The encoder network consists of an encoder embedding layer and a LSTM layer.\n",
        "\n",
        "The decoder network encompasses both decoder and attention mechanism.\n",
        "\n",
        "The example uses LuongAttention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJXNQ4Vh26t9",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlOVNNrCC5_-",
        "colab_type": "text"
      },
      "source": [
        "To begin with, attention mechanism is initialized without memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhtJ4YT46mh",
        "colab_type": "code",
        "outputId": "1ad55193-fa0f-4db8-8413-09c460feca89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8vdgaZFAah",
        "colab_type": "text"
      },
      "source": [
        "### One step of training on a batch using Teacher Forcing technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3HkY9nExNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        s_prev = [a_tx, c_tx]\n",
        "\n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=s_prev,\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_GRb1JHp6b",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "get existing checkpoint objects\n",
        "\n",
        "Object based Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozhQ6j2zIcfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive containing trained checkpoint objects\n",
        "drive.mount('/content/drive', force_remount=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFd3GHYEaeP",
        "colab_type": "text"
      },
      "source": [
        "We load from previously saved checkpoints from Google Drive if already trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbjvgK8AHzHF",
        "colab_type": "code",
        "outputId": "0f5723e5-3685-4f52-ed1c-b87062397f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "checkpointdir = os.path.join('/content/drive/My Drive/DL',\"nmt_tfa_logs_eng_to_fra_withAttention\")\n",
        "chkpoint_prefix = os.path.join(checkpointdir, \"chkpoint\")\n",
        "if not os.path.exists(checkpointdir):\n",
        "    os.mkdir(checkpointdir)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(checkpointdir)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(checkpointdir))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at /content/drive/My Drive/DL/nmt_tfa_logs_eng_to_fra_withAttention/chkpoint-48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMQ9KDTE3s4",
        "colab_type": "code",
        "outputId": "c223ad27-6ac3-481b-d40c-0746ef00b080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "epochs = 1\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%20 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 0.4862474799156189 epoch 1 batch 20 \n",
            "total loss: 0.38561180233955383 epoch 1 batch 40 \n",
            "total loss: 0.3138387203216553 epoch 1 batch 60 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6t5fpSYbt32",
        "colab_type": "text"
      },
      "source": [
        "### Inference\n",
        "Create input sequence to pass to encoder.\n",
        "\n",
        "The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "\n",
        "Stop predicting when the model predicts the end token.\n",
        "\n",
        "And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "159gIIkrFxxF",
        "colab_type": "code",
        "outputId": "05d1e47c-a0fe-4129-e8c1-b9fae9f35f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2374, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buZEhU1KgSTZ",
        "colab_type": "text"
      },
      "source": [
        "if restoring from checkpoint, lets print all variables related to decoder_embeddings and then select and load the right variable containing decoder embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIO4YKPj6Ssx",
        "colab_type": "code",
        "outputId": "fe6f0283-6909-4868-98be-e27d6ede0d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    checkpointdir) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU8z6etzjBg3",
        "colab_type": "code",
        "outputId": "b95e3064-2697-480e-d781-c428e7d937aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2374, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aw0YCHbvo4",
        "colab_type": "code",
        "outputId": "689616c5-cf61-4799-e948-102ce2e40481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"Hi  \\nHow are you today\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "s_prev = [a_tx, c_tx]\n",
        "#output_sequences = []\n",
        "print('a_tx :',a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "print(\"s_prev = [a_tx, c_tx] :\",np.array(s_prev).shape)\n",
        "\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "#finished,start_inputs = greedy_sampler.initialize(decoder_embedding_matrix,start_tokens,end_token)\n",
        "#print(finished.shape, start_inputs.shape)\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=s_prev,\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "                                                                               "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (2, 1024)\n",
            "c_tx : (2, 1024)\n",
            "s_prev = [a_tx, c_tx] : (2, 2, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \n",
            " \n",
            "decoder initial state shape : (6,)\n",
            "decoder_initial_state tensor \n",
            " AttentionWrapperState(cell_state=[<tf.Tensor: id=397, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.53455395,  0.09223434,  0.12856644, ..., -0.06957067,\n",
            "         0.3530848 ,  0.11687483],\n",
            "       [ 0.06639522, -0.04881477, -0.03442283, ...,  0.04615602,\n",
            "         0.06590631, -0.01256901]], dtype=float32)>, <tf.Tensor: id=394, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.85186946,  0.1760425 ,  0.21882483, ..., -0.2103162 ,\n",
            "         0.5550678 ,  0.23059614],\n",
            "       [ 0.27042344, -0.10663059, -0.08398166, ...,  0.11106212,\n",
            "         0.12134853, -0.0282348 ]], dtype=float32)>], attention=<tf.Tensor: id=16902, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: id=16899, shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: id=16898, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: id=16905, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (2, 256)\n",
            "start_index_emb_avg  tf.Tensor(-0.11679596, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuz1hnwH5pK",
        "colab_type": "text"
      },
      "source": [
        "Discard translations on encountering first sequence \\<end\\>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZ-ENBooiNy",
        "colab_type": "code",
        "outputId": "4cace179-437b-401a-b74b-f9655879bd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nFrench Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence:\n",
            "Hi  \n",
            "How are you today\n",
            "\n",
            "French Translation:\n",
            "salut !\n",
            "comment oses tu ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0FlH3sc8sXn",
        "colab_type": "text"
      },
      "source": [
        "### Inference using Beam Search with beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOJe1v-X8u-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "5fc1b2c5-c06b-4b75-f16a-3986f2bf92a0"
      },
      "source": [
        "beam_width = 3\n",
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"Hi  \\nHow are you today\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "s_prev = [a_tx, c_tx]\n",
        "#output_sequences = []\n",
        "\n",
        "\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "\n",
        "#From official documentation\n",
        "#NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "\n",
        "#The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "#The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "#The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "#set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "encoder_state = tfa.seq2seq.tile_batch(s_prev, multiplier=beam_width)\n",
        "decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "for j in range(maximum_iterations):\n",
        "    beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "    scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "    beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "print(predictions.shape) \n",
        "print(beam_scores.shape)                                                                             "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (6, 7, 1024)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (2, 3, 256)\n",
            "(2, 3, 14)\n",
            "(2, 3, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4e836da2-eace-4b5a-e8e5-f25d9f630739",
        "id": "HdzgAX5JRP6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "print(\"-----------------\")\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"-----------------\")\n",
        "print(\"\\nFrench Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=2, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "English Sentence:\n",
            "Hi  \n",
            "How are you today\n",
            "-----------------\n",
            "\n",
            "French Translation:\n",
            "---------------------------------------------\n",
            "salut !  beam score:  -2.2707102\n",
            "bonjour !  beam score:  -3.6652293\n",
            "merci !  beam score:  -5.248112\n",
            "---------------------------------------------\n",
            "comment est vous ?  beam score:  -12.083749\n",
            "vous oses tu ?  beam score:  -13.619988\n",
            "c il nous ?  beam score:  -16.289078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDD7_FqQP2e",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpQFnMTQR9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_step(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # we can do initialization in outer block\n",
        "    #encoder_initial_cell_state = encoder.initialize_initial_state()\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                    initial_state =encoder_initial_cell_state)\n",
        "\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    s_prev = [h_tx, c_tx]\n",
        "\n",
        "    decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "    #compare logits with timestepped +1 version of decoder_input\n",
        "    decoder_output = output_batch[:,1:] #ignore <start>\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(decoderNetwork.rnn_cell, \n",
        "                                                greedy_sampler,\n",
        "                                                decoderNetwork.dense_layer)\n",
        "    #BasicDecoderOutput\n",
        "\n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                       encoder_state=s_prev,\n",
        "                                                                       Dtype=tf.float32)\n",
        "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                           sequence_length=BATCH_SIZE*[Ty-1])\n",
        "    logits = outputs.rnn_output\n",
        "    sample_id = outputs.sample_id\n",
        "    #Calculate loss\n",
        "    loss = loss_function(logits, decoder_output)\n",
        "    return loss, sample_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOJtMbjZsAuh",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Loss on Entire Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkmAtIIrV9-l",
        "colab_type": "code",
        "outputId": "06cf0ae1-4e0d-4130-a231-38b36dd95832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))\n",
        "for (input_batch, output_batch) in dataset_test.take(-1):\n",
        "    batch_size = len(input_batch)\n",
        "    print(input_batch.shape)\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\n",
        "                                  tf.zeros((batch_size, rnn_units))]\n",
        "    loss,_ = eval_step(input_batch, output_batch, encoder_initial_cell_state, batch_size)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 7)\n",
            "Training loss 0.2632395923137665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGvT_1g0pPI",
        "colab_type": "code",
        "outputId": "6d642b90-7d20-4771-d0d9-ee4d530ede8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#BasicDecoder initialization returns the <start> sequence as first_input\n",
        "#Check Inference Cell output\n",
        "\n",
        "start_index = Y_tokenizer.word_index['<start>']\n",
        "start_index = tf.constant([start_index], dtype = tf.int32)\n",
        "print(start_index)\n",
        "start_index_emb = decoderNetwork.decoder_embedding(start_index)\n",
        "print(start_index_emb.shape)\n",
        "start_index_emb_avg = tf.reduce_sum(start_index_emb)\n",
        "print(start_index_emb_avg.numpy()) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "(1, 256)\n",
            "-0.11850953\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}