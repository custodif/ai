{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_to_French_seq2seq_tf_2.0_withAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSYiDemMKxnJ",
        "colab_type": "text"
      },
      "source": [
        "**DISCLAIMER:**\n",
        "This tutorial demostrates a Neural Machine Translation example based on attention model making use of Tensorflow_addons.seq2seq API. The tutorial takes the ideas & code excerpts from the official tensorflow tutorial as presented in following link \n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md\n",
        "\n",
        "https://github.com/tensorflow/nmt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNNBYh0xgeQ",
        "colab_type": "text"
      },
      "source": [
        "### Datasets\n",
        "For French-English Translation\n",
        "\n",
        "http://www.manythings.org/anki/fra-eng.zip\n",
        "\n",
        "For English-Hindi Translation\n",
        "\n",
        "http://www.manythings.org/anki/hin-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo02O1JtJU5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ASKc-qyIbF",
        "colab_type": "code",
        "outputId": "9b5b1bda-a10a-47b9-841e-4d605c044342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udq3BXq10NhZ",
        "colab_type": "text"
      },
      "source": [
        "### Download File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-fvT1cC0Qsd",
        "colab_type": "code",
        "outputId": "5ed83d66-0a55-4579-b0f7-3a660e8a502c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "print(zipfilename)\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)\n",
        "!ls /content/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fra-eng.zip\n",
            "_about.txt  fra-eng.zip  fra.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imc3M7Wb9BQj",
        "colab_type": "code",
        "outputId": "9f6aac69-965e-42b5-ec30-291a0403cdc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /content/fra.txt | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q6Yn0Y18gac",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHmZsuXH3SeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = file.read()\n",
        "    file.close()\n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVM5nnmb81UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3syZHkX83Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For non-english characterset translations such as Hindi, Russian, etc. we keep unicode. \n",
        "def preprocess_sentence(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = s.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "\n",
        "    s = s.rstrip().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGq0hRk9tad",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPv0BItC9wEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(filename, num_samples):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_samples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfQrukoQ9xP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text,Y_text,_  = create_dataset(\"fra.txt\", num_samples=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pzmGOPIypn",
        "colab_type": "code",
        "outputId": "fd3aab63-13e8-4831-fca4-c18d3ea42fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_text[4000:4001])\n",
        "print(Y_text[4000:4001])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> is this love ? <end>',)\n",
            "('<start> est ce de l amour ? <end>',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3x5kdkhUHa",
        "colab_type": "code",
        "outputId": "d96105d7-cb6a-49e7-949a-5e466cb12e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#total samples\n",
        "print(\"Total Samples : \", len(X_text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples :  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbrbbW0uAUVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function to tokenize words into index using inbuild tokenizer vocabulory\n",
        "# important to override filter otherwise it will filter out all punctuation,\n",
        "# plus tabs and line breaks, minus the ' character.\n",
        "def tokenize(input):\n",
        "   tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "   tokenizer.fit_on_texts(input)\n",
        "   sequences = tokenizer.texts_to_sequences(input)\n",
        "  # print(max_len(sequences))\n",
        "   sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "   return  sequences, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vN7jT9Aijw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42RcXojAmdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\n",
        "X , X_tokenizer = tokenize(X_text)\n",
        "Y, Y_tokenizer = tokenize(Y_text)\n",
        "X_train,  X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "Tx = max_len(X)\n",
        "Ty = max_len(Y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhXwcQ7JPMV",
        "colab_type": "code",
        "outputId": "66b92edb-1b2d-450e-a9de-a310e4666848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"Max length English sentence denoted as Tx : \", Tx)\n",
        "print(\"Max length French sentence denoted as Ty: \", Ty)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length English sentence denoted as Tx :  7\n",
            "Max length French sentence denoted as Ty:  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmCJzO38Apcn",
        "colab_type": "code",
        "outputId": "6204d6e8-0eb1-49a5-eae4-2e40da3b1741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X_tokenizer.word_index['<start>'] #'<start>': 2   # tokenize by frequency\n",
        "input_vocab_size = len(X_tokenizer.word_index)+1  # add 1 for 0 sequence character\n",
        "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", input_vocab_size)\n",
        "print(\"output_vocab_size : \" ,output_vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_vocab_size :  1223\n",
            "output_vocab_size :  2374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLXYtWdZBW6a",
        "colab_type": "text"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUu1izSBVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXuxIq6BZuH",
        "colab_type": "code",
        "outputId": "588b9480-5d6e-4460-b567-9c27d1644b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG-SZOvCY8mY",
        "colab_type": "code",
        "outputId": "bbad47ff-e477-4a33-e7a2-6b11ca519dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqFYB-g1HM5o",
        "colab_type": "text"
      },
      "source": [
        "### Creating Encoder-Decoder Model based on tfa.seq2seq module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Swb9mvflqsQ",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxPNQirBtsh",
        "colab_type": "text"
      },
      "source": [
        "The encoder network consists of an encoder embedding layer and a LSTM layer.\n",
        "\n",
        "The decoder network encompasses both decoder and attention mechanism.\n",
        "\n",
        "The example uses LuongAttention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJXNQ4Vh26t9",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJTg36aCckr",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer and Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc7yp0FWEedf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00WV880hoVMc",
        "colab_type": "text"
      },
      "source": [
        "Here, mask is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pT6hAEFEhVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    #skip loss calculation for padding sequences i.e. y = 0 \n",
        "    #[ <start>,How, are, you, today, 0, 0, 0, 0 ....<end>]\n",
        "    #[ 1, 234, 3, 423, 3344, 0, 0 ,0 ,0, 2 ]\n",
        "    # y is a tensor of [batch_size,Ty] . Create a mask when [y=0]\n",
        "    # mask the loss when padding sequence appears in the output sequence\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlOVNNrCC5_-",
        "colab_type": "text"
      },
      "source": [
        "To begin with, attention mechanism is initialized without memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhtJ4YT46mh",
        "colab_type": "code",
        "outputId": "7d6534fb-b236-4f11-ef48-af131f37db6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8vdgaZFAah",
        "colab_type": "text"
      },
      "source": [
        "### One step of training on a batch using Teacher Forcing technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3HkY9nExNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_GRb1JHp6b",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "get existing checkpoint objects\n",
        "\n",
        "Object based Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozhQ6j2zIcfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive containing trained checkpoint objects\n",
        "drive.mount('/content/drive', force_remount=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFd3GHYEaeP",
        "colab_type": "text"
      },
      "source": [
        "We load from previously saved checkpoints from Google Drive if already trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbjvgK8AHzHF",
        "colab_type": "code",
        "outputId": "747ca157-4533-425a-e758-6460d0caa30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "checkpointdir = os.path.join('/content/drive/My Drive/DL',\"nmt_tfa_logs_eng_to_fra_withAttention\")\n",
        "chkpoint_prefix = os.path.join(checkpointdir, \"chkpoint\")\n",
        "if not os.path.exists(checkpointdir):\n",
        "    os.mkdir(checkpointdir)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(checkpointdir)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(checkpointdir))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVX6Hyx0S3tw",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMQ9KDTE3s4",
        "colab_type": "code",
        "outputId": "d75341ac-bc31-48d8-b8cb-766ae00c02f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%20 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.3734610080718994 epoch 1 batch 20 \n",
            "total loss: 1.3649747371673584 epoch 1 batch 40 \n",
            "total loss: 1.2277458906173706 epoch 1 batch 60 \n",
            "total loss: 1.0381067991256714 epoch 2 batch 20 \n",
            "total loss: 1.0618077516555786 epoch 2 batch 40 \n",
            "total loss: 1.0135551691055298 epoch 2 batch 60 \n",
            "total loss: 0.9900044798851013 epoch 3 batch 20 \n",
            "total loss: 0.8830243349075317 epoch 3 batch 40 \n",
            "total loss: 1.094768762588501 epoch 3 batch 60 \n",
            "total loss: 0.8424139022827148 epoch 4 batch 20 \n",
            "total loss: 0.8161934614181519 epoch 4 batch 40 \n",
            "total loss: 0.7870429158210754 epoch 4 batch 60 \n",
            "total loss: 0.7865346074104309 epoch 5 batch 20 \n",
            "total loss: 0.7479299902915955 epoch 5 batch 40 \n",
            "total loss: 0.7507453560829163 epoch 5 batch 60 \n",
            "total loss: 0.5540140271186829 epoch 6 batch 20 \n",
            "total loss: 0.6129180192947388 epoch 6 batch 40 \n",
            "total loss: 1.0343281030654907 epoch 6 batch 60 \n",
            "total loss: 0.6362163424491882 epoch 7 batch 20 \n",
            "total loss: 0.5650568604469299 epoch 7 batch 40 \n",
            "total loss: 0.5393304824829102 epoch 7 batch 60 \n",
            "total loss: 0.49588748812675476 epoch 8 batch 20 \n",
            "total loss: 0.524472713470459 epoch 8 batch 40 \n",
            "total loss: 0.5218554735183716 epoch 8 batch 60 \n",
            "total loss: 0.40052416920661926 epoch 9 batch 20 \n",
            "total loss: 0.43748098611831665 epoch 9 batch 40 \n",
            "total loss: 0.480436772108078 epoch 9 batch 60 \n",
            "total loss: 0.3703254163265228 epoch 10 batch 20 \n",
            "total loss: 0.34635379910469055 epoch 10 batch 40 \n",
            "total loss: 0.41178932785987854 epoch 10 batch 60 \n",
            "total loss: 0.274386465549469 epoch 11 batch 20 \n",
            "total loss: 0.29020729660987854 epoch 11 batch 40 \n",
            "total loss: 0.34468722343444824 epoch 11 batch 60 \n",
            "total loss: 0.28885582089424133 epoch 12 batch 20 \n",
            "total loss: 0.25322800874710083 epoch 12 batch 40 \n",
            "total loss: 0.33623042702674866 epoch 12 batch 60 \n",
            "total loss: 0.2378627508878708 epoch 13 batch 20 \n",
            "total loss: 0.24538882076740265 epoch 13 batch 40 \n",
            "total loss: 0.3099547028541565 epoch 13 batch 60 \n",
            "total loss: 0.1954009234905243 epoch 14 batch 20 \n",
            "total loss: 0.23426532745361328 epoch 14 batch 40 \n",
            "total loss: 0.2534847855567932 epoch 14 batch 60 \n",
            "total loss: 0.18757615983486176 epoch 15 batch 20 \n",
            "total loss: 0.2022065371274948 epoch 15 batch 40 \n",
            "total loss: 0.24859444797039032 epoch 15 batch 60 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6t5fpSYbt32",
        "colab_type": "text"
      },
      "source": [
        "### Inference\n",
        "Create input sequence to pass to encoder.\n",
        "\n",
        "The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "\n",
        "Stop predicting when the model predicts the end token.\n",
        "\n",
        "And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "159gIIkrFxxF",
        "colab_type": "code",
        "outputId": "36c1371d-9628-441d-fe57-ac43e0b0acd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2374, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buZEhU1KgSTZ",
        "colab_type": "text"
      },
      "source": [
        "if restoring from checkpoint, lets print all variables related to decoder_embeddings and then select and load the right variable containing decoder embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIO4YKPj6Ssx",
        "colab_type": "code",
        "outputId": "fe6f0283-6909-4868-98be-e27d6ede0d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    checkpointdir) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU8z6etzjBg3",
        "colab_type": "code",
        "outputId": "b95e3064-2697-480e-d781-c428e7d937aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2374, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aw0YCHbvo4",
        "colab_type": "code",
        "outputId": "f44766fa-7fea-4a4a-e8c6-0e89a3dbe261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"Hi  \\nHow are you today\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "\n",
        "#output_sequences = []\n",
        "print('a_tx :',a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "\n",
        "\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "#finished,start_inputs = greedy_sampler.initialize(decoder_embedding_matrix,start_tokens,end_token)\n",
        "#print(finished.shape, start_inputs.shape)\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array(s_prev).shape)\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=[a_tx, c_tx],\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "                                                                               "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (2, 1024)\n",
            "c_tx : (2, 1024)\n",
            "decoder_initial_state = [a_tx, c_tx] : (2, 2, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \n",
            " \n",
            "decoder initial state shape : (6,)\n",
            "decoder_initial_state tensor \n",
            " AttentionWrapperState(cell_state=[<tf.Tensor: id=2831499, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.33315176,  0.08027603, -0.2464958 , ..., -0.00591025,\n",
            "         0.13828868, -0.39022484],\n",
            "       [-0.11484488,  0.01991144, -0.02326253, ..., -0.00063522,\n",
            "         0.02958429, -0.0209769 ]], dtype=float32)>, <tf.Tensor: id=2831496, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.4640779 ,  0.13793708, -0.3628047 , ..., -0.01452317,\n",
            "         0.21425426, -0.6372464 ],\n",
            "       [-0.22007388,  0.04059494, -0.06102624, ..., -0.00175705,\n",
            "         0.05973196, -0.05578747]], dtype=float32)>], attention=<tf.Tensor: id=2847950, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: id=2847947, shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: id=2847946, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: id=2847953, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (2, 256)\n",
            "start_index_emb_avg  tf.Tensor(-0.5792763, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuz1hnwH5pK",
        "colab_type": "text"
      },
      "source": [
        "Discard translations on encountering first sequence \\<end\\>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZ-ENBooiNy",
        "colab_type": "code",
        "outputId": "4bea0813-3b75-48ef-a33c-0b3ce7c1f4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nFrench Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence:\n",
            "Hi  \n",
            "How are you today\n",
            "\n",
            "French Translation:\n",
            "salut !\n",
            "comment es tu egoiste !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0FlH3sc8sXn",
        "colab_type": "text"
      },
      "source": [
        "### Inference using Beam Search with beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOJe1v-X8u-n",
        "colab_type": "code",
        "outputId": "43f2c1af-a8d7-4298-9c16-e413bb3e5f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "beam_width = 3\n",
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"Hi  \\nHow are you today\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "\n",
        "#From official documentation\n",
        "#NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "\n",
        "#The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "#The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "#The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "#set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "encoder_state = tfa.seq2seq.tile_batch([a_tx, c_tx], multiplier=beam_width)\n",
        "decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "for j in range(maximum_iterations):\n",
        "    beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "    scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "    beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "print(predictions.shape) \n",
        "print(beam_scores.shape)                                                                             "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (6, 7, 1024)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (2, 3, 256)\n",
            "(2, 3, 14)\n",
            "(2, 3, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5d1ad393-7759-44e4-aac6-9ddfea6eb83e",
        "id": "HdzgAX5JRP6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "print(\"-----------------\")\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"-----------------\")\n",
        "print(\"\\nFrench Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=2, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "English Sentence:\n",
            "Hi  \n",
            "How are you today\n",
            "-----------------\n",
            "\n",
            "French Translation:\n",
            "---------------------------------------------\n",
            "salut !  beam score:  -0.2884619\n",
            "adieu !  beam score:  -7.49721\n",
            "sante !  beam score:  -8.574971\n",
            "---------------------------------------------\n",
            "comment es tu egoiste !  beam score:  -7.808234\n",
            "qu etes vous malade !  beam score:  -14.421217\n",
            "t vas malade !  beam score:  -14.999598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDD7_FqQP2e",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpQFnMTQR9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_step(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # we can do initialization in outer block\n",
        "    #encoder_initial_cell_state = encoder.initialize_initial_state()\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                    initial_state =encoder_initial_cell_state)\n",
        "\n",
        "\n",
        "\n",
        "    decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "    #compare logits with timestepped +1 version of decoder_input\n",
        "    decoder_output = output_batch[:,1:] #ignore <start>\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(decoderNetwork.rnn_cell, \n",
        "                                                greedy_sampler,\n",
        "                                                decoderNetwork.dense_layer)\n",
        "    #BasicDecoderOutput\n",
        "\n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    \n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                       encoder_state=[h_tx, c_tx],\n",
        "                                                                       Dtype=tf.float32)\n",
        "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                           sequence_length=BATCH_SIZE*[Ty-1])\n",
        "    logits = outputs.rnn_output\n",
        "    sample_id = outputs.sample_id\n",
        "    #Calculate loss\n",
        "    loss = loss_function(logits, decoder_output)\n",
        "    return loss, sample_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOJtMbjZsAuh",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Loss on Entire Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkmAtIIrV9-l",
        "colab_type": "code",
        "outputId": "882cf098-43cc-413a-d951-2e9cfe54ce68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))\n",
        "for (input_batch, output_batch) in dataset_test.take(-1):\n",
        "    batch_size = len(input_batch)\n",
        "    print(input_batch.shape)\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\n",
        "                                  tf.zeros((batch_size, rnn_units))]\n",
        "    loss,_ = eval_step(input_batch, output_batch, encoder_initial_cell_state, batch_size)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 7)\n",
            "Training loss 0.9802702069282532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGvT_1g0pPI",
        "colab_type": "code",
        "outputId": "acb3f4bb-6f33-4cb6-e6af-669271b8caca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#BasicDecoder initialization returns the <start> sequence as first_input\n",
        "#Check Inference Cell output\n",
        "\n",
        "start_index = Y_tokenizer.word_index['<start>']\n",
        "start_index = tf.constant([start_index], dtype = tf.int32)\n",
        "print(start_index)\n",
        "start_index_emb = decoderNetwork.decoder_embedding(start_index)\n",
        "print(start_index_emb.shape)\n",
        "start_index_emb_avg = tf.reduce_sum(start_index_emb)\n",
        "print(start_index_emb_avg.numpy()) "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "(1, 256)\n",
            "-0.5792763\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}