{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_to_French_seq2seq_tf_2.0_withAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSYiDemMKxnJ",
        "colab_type": "text"
      },
      "source": [
        "**DISCLAIMER:**\n",
        "This tutorial demostrates an example of attention model using Tensorflow addons tfa.seq2seq module. The tutorial takes the design concepts and code excerpts from the official tensorflow tutorial as presented in following link \n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNNBYh0xgeQ",
        "colab_type": "text"
      },
      "source": [
        "### Datasets\n",
        "For French-English Translation\n",
        "\n",
        "http://www.manythings.org/anki/fra-eng.zip\n",
        "\n",
        "For English-Hindi Translation\n",
        "\n",
        "http://www.manythings.org/anki/hin-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo02O1JtJU5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ASKc-qyIbF",
        "colab_type": "code",
        "outputId": "d3d54a24-6bae-4338-c996-f378d2c1dbc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udq3BXq10NhZ",
        "colab_type": "text"
      },
      "source": [
        "### Download File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-fvT1cC0Qsd",
        "colab_type": "code",
        "outputId": "41174e23-86d2-4a11-cee4-93a9c8699621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#zipfile = tf.keras.utils.get_file('hin-eng.zip',origin='http://www.manythings.org/anki/hin-eng.zip', extract=True)\n",
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "print(zipfilename)\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)\n",
        "!ls /content/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fra-eng.zip\n",
            "_about.txt  fra-eng.zip  fra.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imc3M7Wb9BQj",
        "colab_type": "code",
        "outputId": "9f6aac69-965e-42b5-ec30-291a0403cdc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /content/fra.txt | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q6Yn0Y18gac",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHmZsuXH3SeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = file.read()\n",
        "    file.close()\n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVM5nnmb81UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3syZHkX83Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For non-english characterset translations such as Hindi, Russian, etc. we keep unicode. \n",
        "def preprocess_sentence(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = s.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "\n",
        "    s = s.rstrip().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGq0hRk9tad",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPv0BItC9wEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(filename, num_samples):\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, filename)\n",
        "    file = io.open(path,encoding='UTF-8')\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_samples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfQrukoQ9xP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text,Y_text,_  = create_dataset(\"fra.txt\", num_samples=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pzmGOPIypn",
        "colab_type": "code",
        "outputId": "81558bbd-125b-4238-aebe-2b25fc23196a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_text[4000:4001])\n",
        "print(Y_text[4000:4001])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> is this love ? <end>',)\n",
            "('<start> est ce de l amour ? <end>',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3x5kdkhUHa",
        "colab_type": "code",
        "outputId": "a27404ad-6495-49ad-8d3f-5378a3703dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#total samples\n",
        "print(\"Total Samples : \", len(X_text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples :  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbrbbW0uAUVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function to tokenize words into index using inbuild tokenizer vocabulory\n",
        "# important to override filter otherwise it will filter out all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "def tokenize(input):\n",
        "   tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "   tokenizer.fit_on_texts(input)\n",
        "   sequences = tokenizer.texts_to_sequences(input)\n",
        "  # print(max_len(sequences))\n",
        "   sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "   return  sequences, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vN7jT9Aijw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42RcXojAmdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\n",
        "X , X_tokenizer = tokenize(X_text)\n",
        "Y, Y_tokenizer = tokenize(Y_text)\n",
        "X_train,  X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "Tx = max_len(X)\n",
        "Ty = max_len(Y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhXwcQ7JPMV",
        "colab_type": "code",
        "outputId": "1bde34f6-679f-4f13-ee43-dc96f741fb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Max length English sentence : \", Tx)\n",
        "print(\"Max length French sentence : \", Ty)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length English sentence :  7\n",
            "Max length French sentence :  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmCJzO38Apcn",
        "colab_type": "code",
        "outputId": "6099f8bc-efe7-4b34-c2cd-f559c867870f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_tokenizer.word_index['<start>'] #'<start>': 2   # tokenize by frequency\n",
        "input_vocab_size = len(X_tokenizer.word_index)+1  # add 1 for reserve index 0 which is not included in dictionary\n",
        "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
        "print(\"input vocab size : \", input_vocab_size)\n",
        "print(\"output vocab size : \" ,output_vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input vocab size :  1223\n",
            "output vocab size :  2374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLXYtWdZBW6a",
        "colab_type": "text"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUu1izSBVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXuxIq6BZuH",
        "colab_type": "code",
        "outputId": "f0c9d245-84e4-49d4-ad31-f099f16f5c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG-SZOvCY8mY",
        "colab_type": "code",
        "outputId": "2af24a52-1c68-4baf-b367-7e5a3df7fac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7)\n",
            "(64, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqFYB-g1HM5o",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow Addons 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL1e3gcnBqcR",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVX6Hyx0S3tw",
        "colab": {}
      },
      "source": [
        "#for LSTM need to initialite Tx hidden and Tx memory state , for GRU only need one of the field\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJTg36aCckr",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer and Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc7yp0FWEedf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00WV880hoVMc",
        "colab_type": "text"
      },
      "source": [
        "###Important note:\n",
        " It's worth pointing out that we divide the loss by batch_size, so our hyperparameters are \"invariant\" to batch_size. Some people divide the loss by (batch_size * num_time_steps), which plays down the errors made on short sentences. \n",
        "\n",
        "More subtly, our hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of 1.0, the latter approach effectively uses a much smaller learning rate of 1 / num_time_steps.\n",
        "\n",
        "### Here, mask is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pT6hAEFEhVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "    # we are running decoder for each time step, Expected shape of y and y_pred\n",
        "    #shape of y [batch_size, ty] --> [64, 29]\n",
        "    #shape of y_pred [batch_size, output_vocab_size] --> [64,29,4951]\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    #skip loss calculation for padding i.e. y = 0 index is reserved for padding\n",
        "    # y is a tensor of batch_size,1 . Create a mask when y=0\n",
        "\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Swb9mvflqsQ",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJXNQ4Vh26t9",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size, output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "                # calling attention with hidden states\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism, attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhtJ4YT46mh",
        "colab_type": "code",
        "outputId": "fff6d251-ab21-4688-a591-c194dcd0cd13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al5yn3K4Epdw",
        "colab_type": "text"
      },
      "source": [
        "###Training using teacher forcing\n",
        "Creates a callable TensorFlow graph from a Python function.\n",
        "\n",
        "function constructs a callable that executes a TensorFlow graph (tf.Graph) created by tracing the TensorFlow operations in func.\n",
        "\n",
        "This allows the TensorFlow runtime to apply optimizations and exploit parallelism in the computation defined by func."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8vdgaZFAah",
        "colab_type": "text"
      },
      "source": [
        "### One step of training on a batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3HkY9nExNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #pass [ last step activations , last memory_state ] as input to decoder for LSTM\n",
        "        s_prev = [a_tx, c_tx]\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE, encoder_state=s_prev,Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput\n",
        "         \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state, sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "        # remove <start> token from output_Batch..run training exclude <t0>\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_GRb1JHp6b",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "get existing checkpoint objects\n",
        "\n",
        "Object based Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozhQ6j2zIcfM",
        "colab_type": "code",
        "outputId": "a9d564e9-87f0-44fd-87d4-de8a6c86acca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount gdrive containing trained checkpoint objects\n",
        "drive.mount('/content/drive', force_remount=True )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbjvgK8AHzHF",
        "colab_type": "code",
        "outputId": "7cdace8f-2cd6-49db-ecf5-a0eeff61eee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "checkpointdir = os.path.join('/content/drive/My Drive/DL',\"nmt_tfa_logs_eng_to_fra_withAttention\")\n",
        "chkpoint_prefix = os.path.join(checkpointdir, \"chkpoint\")\n",
        "if not os.path.exists(checkpointdir):\n",
        "    os.mkdir(checkpointdir)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(checkpointdir)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(checkpointdir))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at /content/drive/My Drive/DL/nmt_tfa_logs_eng_to_fra_withAttention/chkpoint-45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGvT_1g0pPI",
        "colab_type": "code",
        "outputId": "aa74b329-efd0-4a56-a292-0d7d54b8a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#decoder <start> sequence weight mean\n",
        "start_index = Y_tokenizer.word_index['<start>']\n",
        "start_index = tf.constant([start_index], dtype = tf.int32)\n",
        "print(start_index)\n",
        "start_index_emb = decoderNetwork.decoder_embedding(start_index)\n",
        "print(start_index_emb.shape)\n",
        "start_index_emb_avg = tf.reduce_sum(start_index_emb)\n",
        "print(start_index_emb_avg.numpy()) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "(1, 256)\n",
            "1.2889861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyx7ilvXh562",
        "colab_type": "code",
        "outputId": "802f93fc-5c9c-4e25-ba90-992ce398bbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#check that some variable weights exists if loading from checkpoint\n",
        "print(len(decoderNetwork.variables))  #[output_vocab_size, embedding_dims]\n",
        "print(len(encoderNetwork.variables))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMQ9KDTE3s4",
        "colab_type": "code",
        "outputId": "dd72ebc4-5013-45c1-c035-c9460f403075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%20 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.9033313989639282 epoch 1 batch 20 \n",
            "total loss: 1.6031697988510132 epoch 1 batch 40 \n",
            "total loss: 1.440329909324646 epoch 1 batch 60 \n",
            "total loss: 1.2812906503677368 epoch 2 batch 20 \n",
            "total loss: 1.2377431392669678 epoch 2 batch 40 \n",
            "total loss: 1.3126517534255981 epoch 2 batch 60 \n",
            "total loss: 1.1322230100631714 epoch 3 batch 20 \n",
            "total loss: 1.2145603895187378 epoch 3 batch 40 \n",
            "total loss: 1.2067127227783203 epoch 3 batch 60 \n",
            "total loss: 1.0027304887771606 epoch 4 batch 20 \n",
            "total loss: 0.9286714792251587 epoch 4 batch 40 \n",
            "total loss: 0.9177215695381165 epoch 4 batch 60 \n",
            "total loss: 0.876984179019928 epoch 5 batch 20 \n",
            "total loss: 0.8240584135055542 epoch 5 batch 40 \n",
            "total loss: 0.9926271438598633 epoch 5 batch 60 \n",
            "total loss: 0.6743034720420837 epoch 6 batch 20 \n",
            "total loss: 0.7864662408828735 epoch 6 batch 40 \n",
            "total loss: 0.7916507720947266 epoch 6 batch 60 \n",
            "total loss: 0.6255617737770081 epoch 7 batch 20 \n",
            "total loss: 0.6889328360557556 epoch 7 batch 40 \n",
            "total loss: 0.6486655473709106 epoch 7 batch 60 \n",
            "total loss: 0.5781083106994629 epoch 8 batch 20 \n",
            "total loss: 0.5864031910896301 epoch 8 batch 40 \n",
            "total loss: 0.6139541268348694 epoch 8 batch 60 \n",
            "total loss: 0.5225856304168701 epoch 9 batch 20 \n",
            "total loss: 0.6008531451225281 epoch 9 batch 40 \n",
            "total loss: 0.5482690930366516 epoch 9 batch 60 \n",
            "total loss: 0.41709014773368835 epoch 10 batch 20 \n",
            "total loss: 0.4744033217430115 epoch 10 batch 40 \n",
            "total loss: 0.4323332607746124 epoch 10 batch 60 \n",
            "total loss: 0.345695436000824 epoch 11 batch 20 \n",
            "total loss: 0.4229932427406311 epoch 11 batch 40 \n",
            "total loss: 0.38073182106018066 epoch 11 batch 60 \n",
            "total loss: 0.29366710782051086 epoch 12 batch 20 \n",
            "total loss: 0.3124863803386688 epoch 12 batch 40 \n",
            "total loss: 0.35221385955810547 epoch 12 batch 60 \n",
            "total loss: 0.28555479645729065 epoch 13 batch 20 \n",
            "total loss: 0.31664296984672546 epoch 13 batch 40 \n",
            "total loss: 0.36968499422073364 epoch 13 batch 60 \n",
            "total loss: 0.22876903414726257 epoch 14 batch 20 \n",
            "total loss: 0.2556006610393524 epoch 14 batch 40 \n",
            "total loss: 0.29816317558288574 epoch 14 batch 60 \n",
            "total loss: 0.2401818484067917 epoch 15 batch 20 \n",
            "total loss: 0.2856447696685791 epoch 15 batch 40 \n",
            "total loss: 0.26059892773628235 epoch 15 batch 60 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6t5fpSYbt32",
        "colab_type": "text"
      },
      "source": [
        "### Inference\n",
        "Create input sequence to pass to encoder.\n",
        "\n",
        "The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "\n",
        "Stop predicting when the model predicts the end token.\n",
        "\n",
        "And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "159gIIkrFxxF",
        "colab_type": "code",
        "outputId": "845bdac4-4373-46e0-9d80-764a47968751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "\n",
        "\n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3007, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buZEhU1KgSTZ",
        "colab_type": "text"
      },
      "source": [
        "if restoring from checkpoint, lets print all variables related to decoder_embeddings and then select and load the right variable containing decoder embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIO4YKPj6Ssx",
        "colab_type": "code",
        "outputId": "8a396a42-fa5e-4501-89ed-3ef856ec55e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(checkpointdir) if re.match(r'.*decoder_embedding.*',var[0])  ]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n",
            "('decoderNetwork/decoder_embedding/embeddings/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [2374, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU8z6etzjBg3",
        "colab_type": "code",
        "outputId": "c4b4181f-cafe-4290-ad84-78ffb07dd158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2374, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aw0YCHbvo4",
        "colab_type": "code",
        "outputId": "33309914-2dcd-4484-acbe-2b4a9c2b4493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"Hi  \\nHow are you today\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)), tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, initial_state =encoder_initial_cell_state)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "s_prev = [a_tx, c_tx]\n",
        "#output_sequences = []\n",
        "print('a_tx :',a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "print(\"s_prev = [a_tx, c_tx] :\",np.array(s_prev).shape)\n",
        "\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "#finished, start_inputs = greedy_sampler.initialize(decoder_embedding_matrix,start_tokens, end_token)\n",
        "#print(finished.shape, start_inputs.shape)\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler, output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size, encoder_state=s_prev,Dtype=tf.float32)\n",
        "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens, end_token=end_token, initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "                                                                               "
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (2, 1024)\n",
            "c_tx : (2, 1024)\n",
            "s_prev = [a_tx, c_tx] : (2, 2, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \n",
            " \n",
            "decoder initial state shape : (6,)\n",
            "decoder_initial_state tensor \n",
            " AttentionWrapperState(cell_state=[<tf.Tensor: id=432949, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.50622696,  0.10476793,  0.15953295, ..., -0.03485877,\n",
            "         0.38209227,  0.05028822],\n",
            "       [ 0.05423798, -0.07571906, -0.02596835, ...,  0.03457706,\n",
            "         0.02335815, -0.01079138]], dtype=float32)>, <tf.Tensor: id=432946, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[-0.74728763,  0.19526252,  0.28277415, ..., -0.10372411,\n",
            "         0.5871746 ,  0.10757777],\n",
            "       [ 0.22841427, -0.1665909 , -0.06484494, ...,  0.08734686,\n",
            "         0.04334781, -0.02505926]], dtype=float32)>], attention=<tf.Tensor: id=449400, shape=(2, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: id=449397, shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: id=449396, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: id=449403, shape=(2, 7), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (2, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZ-ENBooiNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3b992614-86af-4c5a-d7f4-851af94a1e3b"
      },
      "source": [
        "\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nFrench Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence:\n",
            "Hi  \n",
            "How are you today\n",
            "\n",
            "French Translation:\n",
            "salut !\n",
            "comment allez vous ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDD7_FqQP2e",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpQFnMTQR9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_step(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # we can do initialization in outer block\n",
        "    #encoder_initial_cell_state = encoder.initialize_initial_state()\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, initial_state =encoder_initial_cell_state)\n",
        "\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    s_prev = [h_tx, c_tx]\n",
        "\n",
        "    decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "    #compare logits with timestepped +1 version of decoder_input\n",
        "    decoder_output = output_batch[:,1:] #ignore <start>\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(decoderNetwork.rnn_cell, greedy_sampler, decoderNetwork.dense_layer)\n",
        "    #BasicDecoderOutput\n",
        "    # Create a sequence_length vector of \n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE, encoder_state=s_prev,Dtype=tf.float32)\n",
        "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state, sequence_length=BATCH_SIZE*[Ty-1])\n",
        "    logits = outputs.rnn_output\n",
        "    sample_id = outputs.sample_id\n",
        "    #Calculate loss\n",
        "    loss = loss_function(logits, decoder_output)\n",
        "    return loss, sample_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOJtMbjZsAuh",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Loss on Entire Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkmAtIIrV9-l",
        "colab_type": "code",
        "outputId": "4b8d6d60-3b0d-4d9f-b9b7-8f53ed1906ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))\n",
        "for (input_batch, output_batch) in dataset_test.take(-1):\n",
        "    batch_size = len(input_batch)\n",
        "    print(input_batch.shape)\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)), tf.zeros((batch_size, rnn_units))]\n",
        "    loss,_ = eval_step(input_batch, output_batch, encoder_initial_cell_state, batch_size)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 7)\n",
            "Training loss 0.33843979239463806\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}