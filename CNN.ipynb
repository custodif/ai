{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirensk/ai/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T-DCXKVaEM5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN classifier on cats vs dogs. Using callbacks functions to resume training\n",
        "\n",
        "\n",
        "**ModelCheckpoint**\n",
        "\n",
        "**EarlyStopping **\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iQN4srTXT1DD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing a Kaggle Dataset"
      ]
    },
    {
      "metadata": {
        "id": "MyiS8tZU4CxK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have an external dataset on a remote server, you can curl/wget the dataset or lets say you want to work directly with dataset on kaggle platform"
      ]
    },
    {
      "metadata": {
        "id": "OH9SuLFjvypS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install kaggle api"
      ]
    },
    {
      "metadata": {
        "id": "wOmQVXM9T4Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1teQREJEXBfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure Kaggle"
      ]
    },
    {
      "metadata": {
        "id": "oO2hMh8SXDcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp '/content/gdrive/My Drive/kaggle.json' ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt2-gQ1eXt-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading and using Kaggle Dataset"
      ]
    },
    {
      "metadata": {
        "id": "1_HU-582XxYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download --unzip dhirensk/cats-vs-dogs-training8000test2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEpXflt_lgsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -d dataset/*\n",
        "!find dataset -type f | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLkuIYSLuKkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using Google Drive to store your Model"
      ]
    },
    {
      "metadata": {
        "id": "sz9SkPnEuU7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount the google drive"
      ]
    },
    {
      "metadata": {
        "id": "XsvDkesducH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gPsvIeqM2xFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### You can also use google drive folder as your dataset source"
      ]
    },
    {
      "metadata": {
        "id": "T360j22osF74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Access google drive base directory by prefixing  **/content/gdrive/'My Drive'/**\n",
        "\n",
        "Copying the google drive file   **/datasets/dataset_cats_dogs_small.tar** into current folder"
      ]
    },
    {
      "metadata": {
        "id": "iZGj1iCbX4-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp /content/gdrive/'My Drive'/datasets/dataset_cats_dogs_small.tar ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DS5txafHotAI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Untar the dataset"
      ]
    },
    {
      "metadata": {
        "id": "eenc8IDresBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xf dataset_cats_dogs_small.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I585no5ufdE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -d dataset/*/*\n",
        "!find dataset -type f | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivVBVTkrRhon",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ]
    },
    {
      "metadata": {
        "id": "e8SRq-dFRkTc",
        "colab_type": "code",
        "outputId": "a9713236-121f-4953-e24f-beca570f6efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D,Flatten,Dense,BatchNormalization, Dropout\n",
        "from keras.optimizers import SGD, Adamax, Adam\n",
        "\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "#Initialize the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "#Adding the layers to CNN\n",
        "#Adding 1st CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), strides=(1,1),padding='same', input_shape=(128,128,3), activation='relu'))\n",
        "#classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Adding 2nd CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 3rd CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 4th CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Flatten the CNN output\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#Adding 1st Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.5))\n",
        "\n",
        "#Adding 2nd Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 3rd Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "#classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding Output layer\n",
        "classifier.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "# Compiling the CNN using adam optimizer\n",
        "classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xl-FVER9c4wT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load existing model weights"
      ]
    },
    {
      "metadata": {
        "id": "ZN3jvbCpgLzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load Existing Model weights and not entire model.\n",
        "if we make changes to the model above then load model will overwrite our changes above\n",
        "so we only add weights and if the weights correspond to old model then we train from beginning\n",
        "we also overwrite the entire saved model when we see improvements in validation loss"
      ]
    },
    {
      "metadata": {
        "id": "LevoDoEg3u8e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/gdrive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_SeYBqxc9VM",
        "colab_type": "code",
        "outputId": "031f95d8-eb23-4efb-da76-802e5c3e49c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Load Existing Model weights\n",
        "import os.path\n",
        "try :\n",
        "    if os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5'):\n",
        "        classifier.load_weights('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5')\n",
        "        print(\"loaded CNN_model.h5 from /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "    else:\n",
        "        print(\"No Model Weights found. Training from beginning\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"The model will be trained from beginning\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded CNN_model.h5 from /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzMeNx0ggXZE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the callback functions"
      ]
    },
    {
      "metadata": {
        "id": "TZHtRjLWgcrd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use 2 callback functions ModelCheckpoint & EarlyStopping. Suppose we had already trained our classifier for 20/25 epochs. Next time if we resume training using saved weights the classifer will not improve much \n",
        "so there is no point in running the classifier for another 25 epochs and hence we include an earlystopping. With ModelCheckpoint we will save the complete model and not just the weights in CNN_model.h5"
      ]
    },
    {
      "metadata": {
        "id": "QMITlpDvhQer",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lets train it for training accuracy instead of validation loss first\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5',monitor='acc',save_best_only=True,verbose=1, save_weights_only=False)\n",
        "callback_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8mbLPKcYqdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use earlystopping later once we have a basic model ready"
      ]
    },
    {
      "metadata": {
        "id": "unPQjYpjYNmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5',monitor='val_loss',save_best_only=True,verbose=1, save_weights_only=False)\n",
        "stopearly = EarlyStopping(monitor='val_loss',min_delta=0, patience=4, verbose=1)\n",
        "callback_list = [checkpoint, stopearly]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSVx-HmCRzof",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "ojzjayIJR-S9",
        "colab_type": "code",
        "outputId": "93c5be1a-1a97-4030-ffbd-afe60a1f52d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "# Fitting the CNN\n",
        "# Image Augmentation to avoid overfitting\n",
        "import time\n",
        "start_time = time.time()\n",
        "#print(start_time)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'dataset/training_set',\n",
        "        target_size=(128, 128),      \n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'dataset/test_set',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "# so as to save history\n",
        "history = classifier.fit_generator(\n",
        "        training_set,\n",
        "        steps_per_epoch=(8000/32),  \n",
        "        epochs=10,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=(2000/32),\n",
        "        callbacks = callback_list)  \n",
        "\n",
        "# print(history.history)\n",
        "elapsed_time = time.time() - start_time\n",
        "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "\n",
        "loss = history.history['loss'][-1]\n",
        "acc = history.history['acc'][-1]\n",
        "print(\"Training Accuracy of last epoch =\" +str(acc))\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_acc = history.history['val_acc'][-1]\n",
        "print('Validation/Test accuracy of last epoch =' + str(val_acc))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 551s 2s/step - loss: 0.1204 - acc: 0.9540 - val_loss: 0.2774 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00001: acc improved from -inf to 0.95400, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 562s 2s/step - loss: 0.1199 - acc: 0.9536 - val_loss: 0.2982 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00002: acc did not improve from 0.95400\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 555s 2s/step - loss: 0.1201 - acc: 0.9516 - val_loss: 0.4770 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00003: acc did not improve from 0.95400\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 571s 2s/step - loss: 0.1120 - acc: 0.9560 - val_loss: 0.3036 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00004: acc improved from 0.95400 to 0.95600, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 575s 2s/step - loss: 0.1103 - acc: 0.9567 - val_loss: 0.2999 - val_acc: 0.8935\n",
            "\n",
            "Epoch 00005: acc improved from 0.95600 to 0.95675, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 572s 2s/step - loss: 0.1070 - acc: 0.9590 - val_loss: 0.4472 - val_acc: 0.8640\n",
            "\n",
            "Epoch 00006: acc improved from 0.95675 to 0.95900, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 552s 2s/step - loss: 0.1100 - acc: 0.9555 - val_loss: 0.3288 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00007: acc did not improve from 0.95900\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 555s 2s/step - loss: 0.1028 - acc: 0.9609 - val_loss: 0.4863 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00008: acc improved from 0.95900 to 0.96088, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 548s 2s/step - loss: 0.0962 - acc: 0.9644 - val_loss: 0.3942 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00009: acc improved from 0.96088 to 0.96437, saving model to /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 550s 2s/step - loss: 0.1000 - acc: 0.9625 - val_loss: 0.3256 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00010: acc did not improve from 0.96437\n",
            "Training Accuracy of last epoch =0.9625\n",
            "Validation/Test accuracy of last epoch =0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ndTfFjufveI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1AKNWdYSCE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model as json and weights in HDF5 format\n",
        "### Not required as we already saved the best model using callbacks"
      ]
    },
    {
      "metadata": {
        "id": "2Im0NwvhSFED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# serialize model to JSON\n",
        "model_json = classifier.to_json()\n",
        "with open(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "classifier.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "print(\"Saved model weights and architecture to disk\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPnxGNRASVLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing on a single image\n",
        "### Try cat_or_dog_1.jpg / cat_or_dog_2.jpg"
      ]
    },
    {
      "metadata": {
        "id": "8-rCw176SbqZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# later...\n",
        "# test on a single test data\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "file = 'dataset/single_prediction/cat_or_dog_2.jpg'\n",
        "Img = img.imread(file)\n",
        "plt.imshow(Img)\n",
        "plt.show()\n",
        "test_image = image.load_img(file,target_size=(64,64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "\n",
        "# add a dimension which represents 'm' for number of examples m =1 for single examples\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "print(test_image.shape)\n",
        "prediction = classifier.predict_classes(test_image)\n",
        "#[[1]] for dog,[[0]] for cat \n",
        "print(prediction)\n",
        "classes = training_set.class_indices\n",
        "#{'cats': 0, 'dogs': 1}\n",
        "for key, value in classes.items():\n",
        "    if (value == int(prediction)):\n",
        "        print(\"predicted class for the test image is : \" + str(key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NL1MPhbkSikj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model back from file and verify that prediction is matching"
      ]
    },
    {
      "metadata": {
        "id": "fQGcMcPwEKoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "# load json and create model\n",
        "from keras.models import model_from_json\n",
        "json_file = open('/content/gdrive/My Drive/Colab Notebooks/CNN_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "# returns an uncompiled model instance\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\"\"\"\n",
        "\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('CNN_model.h5',compile = False)\n",
        "\n",
        "\n",
        "#find the accuracy of training set & test set which was used earlier\n",
        "#You must compile a model before training/testing. Use `model.compile(optimizer, loss)`\n",
        "loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "loaded_model.evaluate(test_image,[[1]])\n",
        "prediction2 = loaded_model.predict(test_image)\n",
        "#loaded_model.summary()\n",
        "for key, value in classes.items():\n",
        "    if (value == int(prediction2)):\n",
        "        print(\"predicted class for the test image is : \" + str(key))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}