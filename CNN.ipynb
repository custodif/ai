{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirensk/ai/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T-DCXKVaEM5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN classifier on cats vs dogs. Using callbacks functions to resume training\n",
        "\n",
        "\n",
        "**ModelCheckpoint**\n",
        "\n",
        "**EarlyStopping **\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iQN4srTXT1DD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing a Kaggle Dataset"
      ]
    },
    {
      "metadata": {
        "id": "MyiS8tZU4CxK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have an external dataset on a remote server, you can curl/wget the dataset or lets say you want to work directly with dataset on kaggle platform"
      ]
    },
    {
      "metadata": {
        "id": "OH9SuLFjvypS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install kaggle api"
      ]
    },
    {
      "metadata": {
        "id": "wOmQVXM9T4Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1teQREJEXBfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure Kaggle"
      ]
    },
    {
      "metadata": {
        "id": "oO2hMh8SXDcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp '/content/gdrive/My Drive/kaggle.json' ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt2-gQ1eXt-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading and using Kaggle Dataset"
      ]
    },
    {
      "metadata": {
        "id": "1_HU-582XxYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download --unzip dhirensk/cats-vs-dogs-training8000test2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEpXflt_lgsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -d dataset/*\n",
        "!find dataset -type f | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLkuIYSLuKkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using Google Drive to store your Model"
      ]
    },
    {
      "metadata": {
        "id": "sz9SkPnEuU7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount the google drive"
      ]
    },
    {
      "metadata": {
        "id": "XsvDkesducH0",
        "colab_type": "code",
        "outputId": "6bffca21-26d6-4d6b-98b9-57a261b9cc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gPsvIeqM2xFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### You can also use google drive folder as your dataset source"
      ]
    },
    {
      "metadata": {
        "id": "T360j22osF74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Access google drive base directory by prefixing  **/content/gdrive/'My Drive'/**\n",
        "\n",
        "Copying the google drive file   **/datasets/dataset_cats_dogs_small.tar** into current folder"
      ]
    },
    {
      "metadata": {
        "id": "iZGj1iCbX4-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp /content/gdrive/'My Drive'/datasets/dataset_cats_dogs_small.tar ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DS5txafHotAI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Untar the dataset"
      ]
    },
    {
      "metadata": {
        "id": "eenc8IDresBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xf dataset_cats_dogs_small.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I585no5ufdE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9e38d640-1614-4ab1-c53f-d982dbce0243"
      },
      "cell_type": "code",
      "source": [
        "!ls -d dataset/*/*\n",
        "!find dataset -type f | wc -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset/single_prediction/cat_or_dog_1.jpg  dataset/test_set/dogs\n",
            "dataset/single_prediction/cat_or_dog_2.jpg  dataset/training_set/cats\n",
            "dataset/test_set/cats\t\t\t    dataset/training_set/dogs\n",
            "10002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ivVBVTkrRhon",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ]
    },
    {
      "metadata": {
        "id": "e8SRq-dFRkTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd29d0fe-5c39-416b-ae1e-e687135e51a7"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D,Flatten,Dense,BatchNormalization, Dropout\n",
        "from keras.optimizers import SGD, Adamax, Adam\n",
        "\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "#Initialize the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "#Adding the layers to CNN\n",
        "#Adding 1st CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), strides=(1,1),padding='same', input_shape=(128,128,3), activation='relu'))\n",
        "#classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Adding 2nd CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 3rd CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 4th CNN layer\n",
        "classifier.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Flatten the CNN output\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#Adding 1st Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.5))\n",
        "\n",
        "#Adding 2nd Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding 3rd Hidden layer\n",
        "classifier.add(Dense(64,activation='relu'))\n",
        "#classifier.add(BatchNormalization())\n",
        "#classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding Output layer\n",
        "classifier.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "# Compiling the CNN using adam optimizer\n",
        "classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xl-FVER9c4wT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load existing model weights"
      ]
    },
    {
      "metadata": {
        "id": "ZN3jvbCpgLzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load Existing Model weights and not entire model.\n",
        "if we make changes to the model above then load model will overwrite our changes above\n",
        "so we only add weights and if the weights correspond to old model then we train from beginning\n",
        "we also overwrite the entire saved model when we see improvements in validation loss"
      ]
    },
    {
      "metadata": {
        "id": "LevoDoEg3u8e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/gdrive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_SeYBqxc9VM",
        "colab_type": "code",
        "outputId": "7b232677-7433-466f-c77f-16f493d7a894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Load Existing Model weights\n",
        "import os.path\n",
        "try :\n",
        "    if os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5'):\n",
        "        classifier.load_weights('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5')\n",
        "        print(\"loaded CNN_model.h5 from /content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "    else:\n",
        "        print(\"No Model Weights found. Training from beginning\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"The model will be trained from beginning\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Model Weights found. Training from beginning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzMeNx0ggXZE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the callback functions"
      ]
    },
    {
      "metadata": {
        "id": "TZHtRjLWgcrd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use 2 callback functions ModelCheckpoint & EarlyStopping. Suppose we had already trained our classifier for 20/25 epochs. Next time if we resume training using saved weights the classifer will not improve much \n",
        "so there is no point in running the classifier for another 25 epochs and hence we include an earlystopping. With ModelCheckpoint we will save the complete model and not just the weights in CNN_model.h5"
      ]
    },
    {
      "metadata": {
        "id": "QMITlpDvhQer",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lets train it for better training accuracy i.e. less Bias. We can worry about improving Variance once we have a basic model ready \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5',monitor='acc',save_best_only=True,verbose=1, save_weights_only=False)\n",
        "callback_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7IQNDegO8w2Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom CSVLogger Callback function"
      ]
    },
    {
      "metadata": {
        "id": "lapUb5OI9O5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CSVLogger output can append to existing log file, however the epoch counts do not continue from where we left. New training session will append rows starting with epoch 0. This is a problem if we are interesting in plotting model performance by epochs. Lets see how to tackle this"
      ]
    },
    {
      "metadata": {
        "id": "9ge-5rN29AVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "902c0810-c1f2-4b6d-df7f-7a96e9d5fe88"
      },
      "cell_type": "code",
      "source": [
        "# We want to keep track of epoch counts\n",
        "import pandas as pd\n",
        "try:\n",
        "    if os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/epoch_logs.csv'):\n",
        "        logfile = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/epoch_logs.csv', header=None)\n",
        "        next_epoch = int(logfile.iloc[-1,0])+1\n",
        "        print(\"next epoch starts from :\", next_epoch)\n",
        "    else:\n",
        "        next_epoch = 0\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "next epoch starts from : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wvmDNWLc-P_I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extending the CSVLogger Callback class to make use of epoch counter we created above"
      ]
    },
    {
      "metadata": {
        "id": "lIDQO_cC-PEs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import CSVLogger\n",
        "class CSVLogger2(CSVLogger):\n",
        "    def __init__(self, filename, separator, append):\n",
        "        super(CSVLogger2, self).__init__(filename, separator, append)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        super(CSVLogger2,self).on_epoch_end(epoch+next_epoch, logs)\n",
        "        \n",
        "csvlogger = CSVLogger2(filename ='/content/gdrive/My Drive/Colab Notebooks/epoch_logs.csv', separator = ',', append=True)\n",
        "\n",
        "callback_list = [checkpoint,csvlogger]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82ynN4-6H6jo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Early Stopping Callback"
      ]
    },
    {
      "metadata": {
        "id": "W8mbLPKcYqdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use earlystopping later once we have a basic model ready"
      ]
    },
    {
      "metadata": {
        "id": "unPQjYpjYNmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "stopearly = EarlyStopping(monitor='val_loss',min_delta=0, patience=4, verbose=1)\n",
        "callback_list = [checkpoint, csvlogger, stopearly]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSVx-HmCRzof",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "ojzjayIJR-S9",
        "colab_type": "code",
        "outputId": "6395240d-101b-4fb9-afb8-a4b8d3103d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Fitting the CNN\n",
        "# Image Augmentation to avoid overfitting\n",
        "import time\n",
        "start_time = time.time()\n",
        "#print(start_time)\n",
        "print(\"epochs completed till now : \",next_epoch)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'dataset/training_set',\n",
        "        target_size=(128, 128),      \n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'dataset/test_set',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "# so as to save history\n",
        "history = classifier.fit_generator(\n",
        "        training_set,\n",
        "        steps_per_epoch=(8000/32),  \n",
        "        epochs=5,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=(2000/32),\n",
        "        callbacks = callback_list)  \n",
        "\n",
        "# print(history.history)\n",
        "elapsed_time = time.time() - start_time\n",
        "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "\n",
        "loss = history.history['loss'][-1]\n",
        "acc = history.history['acc'][-1]\n",
        "print(\"Training Accuracy of last epoch =\" +str(acc))\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_acc = history.history['val_acc'][-1]\n",
        "print('Validation/Test accuracy of last epoch =' + str(val_acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs completed till now :  5\n",
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "210/250 [========================>.....] - ETA: 1:21 - loss: 0.5298 - acc: 0.7321"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ndTfFjufveI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1AKNWdYSCE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model as json and weights in HDF5 format\n",
        "### Not required as we already saved the best model using callbacks"
      ]
    },
    {
      "metadata": {
        "id": "2Im0NwvhSFED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# serialize model to JSON\n",
        "model_json = classifier.to_json()\n",
        "with open(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "classifier.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "print(\"Saved model weights and architecture to disk\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPnxGNRASVLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing on a single image\n",
        "### Try cat_or_dog_1.jpg / cat_or_dog_2.jpg"
      ]
    },
    {
      "metadata": {
        "id": "8-rCw176SbqZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# later...\n",
        "# test on a single test data\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "file = 'dataset/single_prediction/cat_or_dog_2.jpg'\n",
        "Img = img.imread(file)\n",
        "plt.imshow(Img)\n",
        "plt.show()\n",
        "test_image = image.load_img(file,target_size=(64,64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "\n",
        "# add a dimension which represents 'm' for number of examples m =1 for single examples\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "print(test_image.shape)\n",
        "prediction = classifier.predict_classes(test_image)\n",
        "#[[1]] for dog,[[0]] for cat \n",
        "print(prediction)\n",
        "classes = training_set.class_indices\n",
        "#{'cats': 0, 'dogs': 1}\n",
        "for key, value in classes.items():\n",
        "    if (value == int(prediction)):\n",
        "        print(\"predicted class for the test image is : \" + str(key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NL1MPhbkSikj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model back from file and verify that prediction is matching"
      ]
    },
    {
      "metadata": {
        "id": "fQGcMcPwEKoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "# load json and create model\n",
        "from keras.models import model_from_json\n",
        "json_file = open('/content/gdrive/My Drive/Colab Notebooks/CNN_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "# returns an uncompiled model instance\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/CNN_model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\"\"\"\n",
        "\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('CNN_model.h5',compile = False)\n",
        "\n",
        "\n",
        "#find the accuracy of training set & test set which was used earlier\n",
        "#You must compile a model before training/testing. Use `model.compile(optimizer, loss)`\n",
        "loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "loaded_model.evaluate(test_image,[[1]])\n",
        "prediction2 = loaded_model.predict(test_image)\n",
        "#loaded_model.summary()\n",
        "for key, value in classes.items():\n",
        "    if (value == int(prediction2)):\n",
        "        print(\"predicted class for the test image is : \" + str(key))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}